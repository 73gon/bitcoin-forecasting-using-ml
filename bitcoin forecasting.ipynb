{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b0d6b0",
   "metadata": {},
   "source": [
    "### installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "251b5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "import praw\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286483a",
   "metadata": {},
   "source": [
    "### fetch bitcoin data from 2017 to 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binance_data(symbol, interval, start_str, end_str=None):\n",
    "    base_url = \"https://api.binance.com\"\n",
    "    endpoint = \"/api/v3/klines\"\n",
    "    \n",
    "    start_ts = int(pd.to_datetime(start_str).timestamp() * 1000)\n",
    "    end_ts = int(pd.to_datetime(end_str).timestamp() * 1000) if end_str else None\n",
    "    \n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'interval': interval,\n",
    "            'startTime': start_ts,\n",
    "            'limit': 1000\n",
    "        }\n",
    "        if end_ts:\n",
    "            params['endTime'] = end_ts\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url + endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            results = response.json()\n",
    "            \n",
    "            if not results:\n",
    "                break\n",
    "            \n",
    "            data.extend(results)\n",
    "            \n",
    "            start_ts = results[-1][0] + 1\n",
    "            \n",
    "            count += len(results)\n",
    "            last_timestamp = pd.to_datetime(results[-1][0], unit='ms')\n",
    "            print(f\"Fetched {count} records, up to {last_timestamp}\")\n",
    "            \n",
    "            if len(results) < 1000:\n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit exceeded. Waiting before retrying...\")\n",
    "                time.sleep(60)  # Wait 1 minute before retrying\n",
    "            else:\n",
    "                raise err\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    columns = [\n",
    "        'open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', \n",
    "        'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', \n",
    "        'taker_buy_quote_asset_volume', 'ignore'\n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    symbol = \"BTCUSDT\"\n",
    "    interval = \"1m\"\n",
    "    start_date = \"2010-01-01\"\n",
    "    end_date = \"2024-05-27\"\n",
    "    \n",
    "    print(\"Starting data fetch...\")\n",
    "    data = get_binance_data(symbol, interval, start_date, end_date)\n",
    "    print(\"Data fetch complete. Saving to CSV...\")\n",
    "    save_to_csv(data, 'bitcoin_dataset.csv')\n",
    "    print(\"Data saved to bitcoin_1min_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c8d57",
   "metadata": {},
   "source": [
    "### load transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a83d64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts processed: 146\n"
     ]
    }
   ],
   "source": [
    "# Initialize sentiment analysis model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "max_length = 512  # Roberta's maximum sequence length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b620cc",
   "metadata": {},
   "source": [
    "### configuration of reddit api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ff3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit API credentials\n",
    "client_id = '-DikpAFUeeajlFFyWTBqUg'\n",
    "client_secret = 'yZu1rPuBINVPuI7LuugQybGwtX1Cdg'\n",
    "user_agent = 'bitcoin data'\n",
    "username = '73malik'\n",
    "password = 'Stapler437581'\n",
    "\n",
    "# Create Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    username=username,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "# Subreddit and Keyword definition\n",
    "subreddit_name = 'all'\n",
    "keywords = ['bitcoin price prediction', 'bitcoin market analysis', 'bitcoin trend', 'bitcoin forecast', 'bitcoin news']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd389b5",
   "metadata": {},
   "source": [
    "### retrieve reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and analyze Reddit posts\n",
    "def fetch_reddit_posts(subreddit_name, keywords, total_limit=1000, score_threshold=10):\n",
    "    posts = []\n",
    "    counter = 0\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        for submission in reddit.subreddit(subreddit_name).search(keyword, sort='new', limit=total_limit):\n",
    "            if counter >= total_limit:\n",
    "                break\n",
    "            if submission.score > score_threshold:\n",
    "                created_date = datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                text = submission.title + \" \" + submission.selftext\n",
    "                truncated_text = tokenizer.decode(tokenizer(text, truncation=True, max_length=max_length)[\"input_ids\"], skip_special_tokens=True)\n",
    "                \n",
    "                # Get the sentiment\n",
    "                sentiment = sentiment_pipeline(truncated_text)[0]['label']\n",
    "                \n",
    "                posts.append({\n",
    "                    'created': created_date,\n",
    "                    'title': submission.title,\n",
    "                    'score': submission.score,\n",
    "                    'url': submission.url,\n",
    "                    'content': submission.selftext,\n",
    "                    'sentiment': sentiment\n",
    "                })\n",
    "                counter += 1\n",
    "                \n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "posts_df = fetch_reddit_posts(subreddit_name, keywords)\n",
    "posts_df.to_csv('reddit_posts.csv', index=False)\n",
    "print(f\"Total posts processed: {len(posts_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f67369",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88424571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame contains NaN values. Filling NaN values with forward fill.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6_/gl6t20496732dk15d6h287jr0000gn/T/ipykernel_28052/2440315738.py:25: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_df.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data shapes: x=(3555069, 60, 2), y=(3555069,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(price_df, sentiment_df, sequence_length=60):\n",
    "    # Convert timestamps to datetime\n",
    "    sentiment_df['created'] = pd.to_datetime(sentiment_df['created'])\n",
    "    price_df['timestamp'] = pd.to_datetime(price_df['open_time'])\n",
    "\n",
    "    # Ensure both DataFrames are sorted by their respective keys\n",
    "    price_df = price_df.sort_values('timestamp')\n",
    "    sentiment_df = sentiment_df.sort_values('created')\n",
    "\n",
    "    # Merge price and sentiment data on timestamp\n",
    "    merged_df = pd.merge_asof(price_df, sentiment_df, left_on='timestamp', right_on='created', direction='backward')\n",
    "\n",
    "    # Fill missing sentiment values with neutral sentiment\n",
    "    merged_df['sentiment'] = merged_df['sentiment'].fillna('neutral')\n",
    "    sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    merged_df['sentiment_score'] = merged_df['sentiment'].map(sentiment_mapping)\n",
    "    \n",
    "    # Normalize 'close' prices\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    merged_df['close'] = scaler.fit_transform(merged_df['close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Check for NaN values in merged DataFrame\n",
    "    if merged_df.isnull().values.any():\n",
    "        print(\"Merged DataFrame contains NaN values. Filling NaN values with forward fill.\")\n",
    "        merged_df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    data = merged_df[['close', 'sentiment_score']].values\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length][0])  # Price prediction target\n",
    "\n",
    "    x, y = np.array(x), np.array(y)\n",
    "\n",
    "    # Final check for NaN and infinite values\n",
    "    if np.any(np.isnan(x)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Prepared data contains NaN values after processing.\")\n",
    "    if np.any(np.isinf(x)) or np.any(np.isinf(y)):\n",
    "        raise ValueError(\"Prepared data contains infinite values after processing.\")\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Load the price and sentiment data\n",
    "price_df = pd.read_csv('bitcoin_dataset.csv')\n",
    "sentiment_df = pd.read_csv('reddit_posts.csv')\n",
    "\n",
    "# Prepare the data\n",
    "x, y = prepare_data(price_df, sentiment_df)\n",
    "print(f\"Prepared data shapes: x={x.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3531e3",
   "metadata": {},
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a368799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data shapes: x=(3555069, 60, 2), y=(3555069,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(price_df, sentiment_df):\n",
    "    # Convert timestamps to datetime\n",
    "    sentiment_df['created'] = pd.to_datetime(sentiment_df['created'])\n",
    "    price_df['timestamp'] = pd.to_datetime(price_df['open_time'])\n",
    "\n",
    "    # Ensure both DataFrames are sorted by their respective keys\n",
    "    price_df = price_df.sort_values('timestamp')\n",
    "    sentiment_df = sentiment_df.sort_values('created')\n",
    "\n",
    "    # Merge price and sentiment data on timestamp\n",
    "    merged_df = pd.merge_asof(price_df, sentiment_df, left_on='timestamp', right_on='created', direction='backward')\n",
    "\n",
    "    # Fill missing sentiment values with neutral sentiment\n",
    "    merged_df['sentiment'] = merged_df['sentiment'].fillna('neutral')\n",
    "    sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    merged_df['sentiment_score'] = merged_df['sentiment'].map(sentiment_mapping)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    sequence_length = 60  # e.g., last 60 minutes\n",
    "    data = merged_df[['close', 'sentiment_score']].values\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        x.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length][0])  # Price prediction target\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Load the price and sentiment data\n",
    "price_df = pd.read_csv('bitcoin_dataset.csv')\n",
    "sentiment_df = pd.read_csv('reddit_posts.csv')\n",
    "\n",
    "# Prepare the data\n",
    "x, y = prepare_data(price_df, sentiment_df)\n",
    "print(f\"Prepared data shapes: x={x.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bbb71",
   "metadata": {},
   "source": [
    "### build and train lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef7039f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Num GPUs Available:  0\n",
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "# Print TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check for GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "# If GPU is available, set memory growth\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        print(f\"Using GPU: {physical_devices[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "975eeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)  # Reduced learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c2d6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the LSTM model\n",
    "input_shape = (x.shape[1], x.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19ff07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "lstm_model.fit(x, y, batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d395256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "lstm_model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1004a5",
   "metadata": {},
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)  # Reduced learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Build the LSTM model\n",
    "input_shape = (x.shape[1], x.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(x, y, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Save the trained model\n",
    "lstm_model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f15d4e",
   "metadata": {},
   "source": [
    "### predict future prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ccf23e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'lstm_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the trained model (if needed)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m----> 3\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predict future prices\u001b[39;00m\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mpredict(x)\n",
      "File \u001b[0;32m~/Desktop/uni/6. Semester - SoSe 2024/Bachelorarbeit/bachelor project /.venv/lib/python3.11/site-packages/keras/src/saving/saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    177\u001b[0m         filepath,\n\u001b[1;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/uni/6. Semester - SoSe 2024/Bachelorarbeit/bachelor project /.venv/lib/python3.11/site-packages/keras/src/legacy/saving/legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[0;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[0;32m~/Desktop/uni/6. Semester - SoSe 2024/Bachelorarbeit/bachelor project /.venv/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Desktop/uni/6. Semester - SoSe 2024/Bachelorarbeit/bachelor project /.venv/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'lstm_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load the trained model (if needed)\n",
    "lstm_model = load_model('lstm_model.h5')\n",
    "\n",
    "# Predict future prices\n",
    "predictions = lstm_model.predict(x)\n",
    "\n",
    "# Compare predictions with actual prices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y, color='blue', label='Actual Bitcoin Price')\n",
    "plt.plot(predictions, color='red', label='Predicted Bitcoin Price')\n",
    "plt.title('Bitcoin Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
